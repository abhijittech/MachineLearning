{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = londonDf = pd.read_csv('train_london.csv',header=None)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.299403</td>\n",
       "      <td>-1.226624</td>\n",
       "      <td>1.498425</td>\n",
       "      <td>-1.176150</td>\n",
       "      <td>5.289853</td>\n",
       "      <td>0.208297</td>\n",
       "      <td>2.404498</td>\n",
       "      <td>1.594506</td>\n",
       "      <td>-0.051608</td>\n",
       "      <td>0.663234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.850465</td>\n",
       "      <td>-0.622990</td>\n",
       "      <td>-1.833057</td>\n",
       "      <td>0.293024</td>\n",
       "      <td>3.552681</td>\n",
       "      <td>0.717611</td>\n",
       "      <td>3.305972</td>\n",
       "      <td>-2.715559</td>\n",
       "      <td>-2.682409</td>\n",
       "      <td>0.101050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.174176</td>\n",
       "      <td>0.332157</td>\n",
       "      <td>0.949919</td>\n",
       "      <td>-1.285328</td>\n",
       "      <td>2.199061</td>\n",
       "      <td>-0.151268</td>\n",
       "      <td>-0.427039</td>\n",
       "      <td>2.619246</td>\n",
       "      <td>-0.765884</td>\n",
       "      <td>-0.093780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.819750</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>2.038836</td>\n",
       "      <td>0.468579</td>\n",
       "      <td>-0.517657</td>\n",
       "      <td>0.422326</td>\n",
       "      <td>0.803699</td>\n",
       "      <td>1.213219</td>\n",
       "      <td>1.382932</td>\n",
       "      <td>-1.817761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.192222</td>\n",
       "      <td>-0.414371</td>\n",
       "      <td>0.067054</td>\n",
       "      <td>-2.233568</td>\n",
       "      <td>3.658881</td>\n",
       "      <td>0.089007</td>\n",
       "      <td>0.203439</td>\n",
       "      <td>-4.219054</td>\n",
       "      <td>-1.184919</td>\n",
       "      <td>-1.240310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.604501</td>\n",
       "      <td>0.750054</td>\n",
       "      <td>-3.360521</td>\n",
       "      <td>0.856988</td>\n",
       "      <td>-2.751451</td>\n",
       "      <td>-1.582735</td>\n",
       "      <td>1.672246</td>\n",
       "      <td>0.656438</td>\n",
       "      <td>-0.932473</td>\n",
       "      <td>2.987436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.573270</td>\n",
       "      <td>-0.580318</td>\n",
       "      <td>-0.866332</td>\n",
       "      <td>-0.603812</td>\n",
       "      <td>3.125716</td>\n",
       "      <td>0.870321</td>\n",
       "      <td>-0.161992</td>\n",
       "      <td>4.499666</td>\n",
       "      <td>1.038741</td>\n",
       "      <td>-1.092716</td>\n",
       "      <td>...</td>\n",
       "      <td>1.022959</td>\n",
       "      <td>1.275598</td>\n",
       "      <td>-3.480110</td>\n",
       "      <td>-1.065252</td>\n",
       "      <td>2.153133</td>\n",
       "      <td>1.563539</td>\n",
       "      <td>2.767117</td>\n",
       "      <td>0.215748</td>\n",
       "      <td>0.619645</td>\n",
       "      <td>1.883397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.613071</td>\n",
       "      <td>-0.644204</td>\n",
       "      <td>1.112558</td>\n",
       "      <td>-0.032397</td>\n",
       "      <td>3.490142</td>\n",
       "      <td>-0.011935</td>\n",
       "      <td>1.443521</td>\n",
       "      <td>-4.290282</td>\n",
       "      <td>-1.761308</td>\n",
       "      <td>0.807652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513906</td>\n",
       "      <td>-1.803473</td>\n",
       "      <td>0.518579</td>\n",
       "      <td>-0.205029</td>\n",
       "      <td>-4.744566</td>\n",
       "      <td>-1.520015</td>\n",
       "      <td>1.830651</td>\n",
       "      <td>0.870772</td>\n",
       "      <td>-1.894609</td>\n",
       "      <td>0.408332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.299403 -1.226624  1.498425 -1.176150  5.289853  0.208297  2.404498   \n",
       "1 -1.174176  0.332157  0.949919 -1.285328  2.199061 -0.151268 -0.427039   \n",
       "2  1.192222 -0.414371  0.067054 -2.233568  3.658881  0.089007  0.203439   \n",
       "3  1.573270 -0.580318 -0.866332 -0.603812  3.125716  0.870321 -0.161992   \n",
       "4 -0.613071 -0.644204  1.112558 -0.032397  3.490142 -0.011935  1.443521   \n",
       "\n",
       "         7         8         9     ...           30        31        32  \\\n",
       "0  1.594506 -0.051608  0.663234    ...    -0.850465 -0.622990 -1.833057   \n",
       "1  2.619246 -0.765884 -0.093780    ...    -0.819750  0.012037  2.038836   \n",
       "2 -4.219054 -1.184919 -1.240310    ...    -0.604501  0.750054 -3.360521   \n",
       "3  4.499666  1.038741 -1.092716    ...     1.022959  1.275598 -3.480110   \n",
       "4 -4.290282 -1.761308  0.807652    ...     0.513906 -1.803473  0.518579   \n",
       "\n",
       "         33        34        35        36        37        38        39  \n",
       "0  0.293024  3.552681  0.717611  3.305972 -2.715559 -2.682409  0.101050  \n",
       "1  0.468579 -0.517657  0.422326  0.803699  1.213219  1.382932 -1.817761  \n",
       "2  0.856988 -2.751451 -1.582735  1.672246  0.656438 -0.932473  2.987436  \n",
       "3 -1.065252  2.153133  1.563539  2.767117  0.215748  0.619645  1.883397  \n",
       "4 -0.205029 -4.744566 -1.520015  1.830651  0.870772 -1.894609  0.408332  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "londonDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = out= pd.read_csv('trainLabels.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.299403</td>\n",
       "      <td>-1.226624</td>\n",
       "      <td>1.498425</td>\n",
       "      <td>-1.176150</td>\n",
       "      <td>5.289853</td>\n",
       "      <td>0.208297</td>\n",
       "      <td>2.404498</td>\n",
       "      <td>1.594506</td>\n",
       "      <td>-0.051608</td>\n",
       "      <td>0.663234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.850465</td>\n",
       "      <td>-0.622990</td>\n",
       "      <td>-1.833057</td>\n",
       "      <td>0.293024</td>\n",
       "      <td>3.552681</td>\n",
       "      <td>0.717611</td>\n",
       "      <td>3.305972</td>\n",
       "      <td>-2.715559</td>\n",
       "      <td>-2.682409</td>\n",
       "      <td>0.101050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.174176</td>\n",
       "      <td>0.332157</td>\n",
       "      <td>0.949919</td>\n",
       "      <td>-1.285328</td>\n",
       "      <td>2.199061</td>\n",
       "      <td>-0.151268</td>\n",
       "      <td>-0.427039</td>\n",
       "      <td>2.619246</td>\n",
       "      <td>-0.765884</td>\n",
       "      <td>-0.093780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.819750</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>2.038836</td>\n",
       "      <td>0.468579</td>\n",
       "      <td>-0.517657</td>\n",
       "      <td>0.422326</td>\n",
       "      <td>0.803699</td>\n",
       "      <td>1.213219</td>\n",
       "      <td>1.382932</td>\n",
       "      <td>-1.817761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.192222</td>\n",
       "      <td>-0.414371</td>\n",
       "      <td>0.067054</td>\n",
       "      <td>-2.233568</td>\n",
       "      <td>3.658881</td>\n",
       "      <td>0.089007</td>\n",
       "      <td>0.203439</td>\n",
       "      <td>-4.219054</td>\n",
       "      <td>-1.184919</td>\n",
       "      <td>-1.240310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.604501</td>\n",
       "      <td>0.750054</td>\n",
       "      <td>-3.360521</td>\n",
       "      <td>0.856988</td>\n",
       "      <td>-2.751451</td>\n",
       "      <td>-1.582735</td>\n",
       "      <td>1.672246</td>\n",
       "      <td>0.656438</td>\n",
       "      <td>-0.932473</td>\n",
       "      <td>2.987436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.573270</td>\n",
       "      <td>-0.580318</td>\n",
       "      <td>-0.866332</td>\n",
       "      <td>-0.603812</td>\n",
       "      <td>3.125716</td>\n",
       "      <td>0.870321</td>\n",
       "      <td>-0.161992</td>\n",
       "      <td>4.499666</td>\n",
       "      <td>1.038741</td>\n",
       "      <td>-1.092716</td>\n",
       "      <td>...</td>\n",
       "      <td>1.022959</td>\n",
       "      <td>1.275598</td>\n",
       "      <td>-3.480110</td>\n",
       "      <td>-1.065252</td>\n",
       "      <td>2.153133</td>\n",
       "      <td>1.563539</td>\n",
       "      <td>2.767117</td>\n",
       "      <td>0.215748</td>\n",
       "      <td>0.619645</td>\n",
       "      <td>1.883397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.613071</td>\n",
       "      <td>-0.644204</td>\n",
       "      <td>1.112558</td>\n",
       "      <td>-0.032397</td>\n",
       "      <td>3.490142</td>\n",
       "      <td>-0.011935</td>\n",
       "      <td>1.443521</td>\n",
       "      <td>-4.290282</td>\n",
       "      <td>-1.761308</td>\n",
       "      <td>0.807652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513906</td>\n",
       "      <td>-1.803473</td>\n",
       "      <td>0.518579</td>\n",
       "      <td>-0.205029</td>\n",
       "      <td>-4.744566</td>\n",
       "      <td>-1.520015</td>\n",
       "      <td>1.830651</td>\n",
       "      <td>0.870772</td>\n",
       "      <td>-1.894609</td>\n",
       "      <td>0.408332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.299403 -1.226624  1.498425 -1.176150  5.289853  0.208297  2.404498   \n",
       "1 -1.174176  0.332157  0.949919 -1.285328  2.199061 -0.151268 -0.427039   \n",
       "2  1.192222 -0.414371  0.067054 -2.233568  3.658881  0.089007  0.203439   \n",
       "3  1.573270 -0.580318 -0.866332 -0.603812  3.125716  0.870321 -0.161992   \n",
       "4 -0.613071 -0.644204  1.112558 -0.032397  3.490142 -0.011935  1.443521   \n",
       "\n",
       "         7         8         9     ...           30        31        32  \\\n",
       "0  1.594506 -0.051608  0.663234    ...    -0.850465 -0.622990 -1.833057   \n",
       "1  2.619246 -0.765884 -0.093780    ...    -0.819750  0.012037  2.038836   \n",
       "2 -4.219054 -1.184919 -1.240310    ...    -0.604501  0.750054 -3.360521   \n",
       "3  4.499666  1.038741 -1.092716    ...     1.022959  1.275598 -3.480110   \n",
       "4 -4.290282 -1.761308  0.807652    ...     0.513906 -1.803473  0.518579   \n",
       "\n",
       "         33        34        35        36        37        38        39  \n",
       "0  0.293024  3.552681  0.717611  3.305972 -2.715559 -2.682409  0.101050  \n",
       "1  0.468579 -0.517657  0.422326  0.803699  1.213219  1.382932 -1.817761  \n",
       "2  0.856988 -2.751451 -1.582735  1.672246  0.656438 -0.932473  2.987436  \n",
       "3 -1.065252  2.153133  1.563539  2.767117  0.215748  0.619645  1.883397  \n",
       "4 -0.205029 -4.744566 -1.520015  1.830651  0.870772 -1.894609  0.408332  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "londonDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 40 columns):\n",
      "0     1000 non-null float64\n",
      "1     1000 non-null float64\n",
      "2     1000 non-null float64\n",
      "3     1000 non-null float64\n",
      "4     1000 non-null float64\n",
      "5     1000 non-null float64\n",
      "6     1000 non-null float64\n",
      "7     1000 non-null float64\n",
      "8     1000 non-null float64\n",
      "9     1000 non-null float64\n",
      "10    1000 non-null float64\n",
      "11    1000 non-null float64\n",
      "12    1000 non-null float64\n",
      "13    1000 non-null float64\n",
      "14    1000 non-null float64\n",
      "15    1000 non-null float64\n",
      "16    1000 non-null float64\n",
      "17    1000 non-null float64\n",
      "18    1000 non-null float64\n",
      "19    1000 non-null float64\n",
      "20    1000 non-null float64\n",
      "21    1000 non-null float64\n",
      "22    1000 non-null float64\n",
      "23    1000 non-null float64\n",
      "24    1000 non-null float64\n",
      "25    1000 non-null float64\n",
      "26    1000 non-null float64\n",
      "27    1000 non-null float64\n",
      "28    1000 non-null float64\n",
      "29    1000 non-null float64\n",
      "30    1000 non-null float64\n",
      "31    1000 non-null float64\n",
      "32    1000 non-null float64\n",
      "33    1000 non-null float64\n",
      "34    1000 non-null float64\n",
      "35    1000 non-null float64\n",
      "36    1000 non-null float64\n",
      "37    1000 non-null float64\n",
      "38    1000 non-null float64\n",
      "39    1000 non-null float64\n",
      "dtypes: float64(40)\n",
      "memory usage: 312.6 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\unzipped\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\unzipped\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "log = LogisticRegression()\n",
    "gnb = GaussianNB()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 123442)\n",
    "\n",
    "gnb.fit(X_train,Y_train)\n",
    "log.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "Y_pred = log.predict(X_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression : Accuracy is  0.808\n"
     ]
    }
   ],
   "source": [
    "print 'Logistic regression : Accuracy is ', metrics.accuracy_score(Y_test,Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_gb_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussion regression : Accuracy is  0.816\n"
     ]
    }
   ],
   "source": [
    "print 'Gaussion regression : Accuracy is ', metrics.accuracy_score(Y_test,Y_gb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_testData = log.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X =  999\n",
      "Length of Y =  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0xfec7160>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD0CAYAAAC7KMweAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf928dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print 'Length of X = ',len(X)\n",
    "print 'Length of Y = ',len(Y)\n",
    "plt.scatter(X['0.29940251144353242'],Y,c=Y,cmap=plt.cm.Dark2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check the models with multiple regressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.299403 -1.226624  1.498425 -1.176150  5.289853  0.208297  2.404498   \n",
      "1 -1.174176  0.332157  0.949919 -1.285328  2.199061 -0.151268 -0.427039   \n",
      "2  1.192222 -0.414371  0.067054 -2.233568  3.658881  0.089007  0.203439   \n",
      "3  1.573270 -0.580318 -0.866332 -0.603812  3.125716  0.870321 -0.161992   \n",
      "4 -0.613071 -0.644204  1.112558 -0.032397  3.490142 -0.011935  1.443521   \n",
      "\n",
      "         7         8         9     ...           30        31        32  \\\n",
      "0  1.594506 -0.051608  0.663234    ...    -0.850465 -0.622990 -1.833057   \n",
      "1  2.619246 -0.765884 -0.093780    ...    -0.819750  0.012037  2.038836   \n",
      "2 -4.219054 -1.184919 -1.240310    ...    -0.604501  0.750054 -3.360521   \n",
      "3  4.499666  1.038741 -1.092716    ...     1.022959  1.275598 -3.480110   \n",
      "4 -4.290282 -1.761308  0.807652    ...     0.513906 -1.803473  0.518579   \n",
      "\n",
      "         33        34        35        36        37        38        39  \n",
      "0  0.293024  3.552681  0.717611  3.305972 -2.715559 -2.682409  0.101050  \n",
      "1  0.468579 -0.517657  0.422326  0.803699  1.213219  1.382932 -1.817761  \n",
      "2  0.856988 -2.751451 -1.582735  1.672246  0.656438 -0.932473  2.987436  \n",
      "3 -1.065252  2.153133  1.563539  2.767117  0.215748  0.619645  1.883397  \n",
      "4 -0.205029 -4.744566 -1.520015  1.830651  0.870772 -1.894609  0.408332  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#Get columns 0 and 1 ie two columns and all rows\n",
    "#X1 = londonDf.iloc[:,:2]\n",
    "\n",
    "X1 = londonDf\n",
    "print X1.head()\n",
    "print len(X1)\n",
    "Y1 = Y\n",
    "print len(Y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>0</td>        <th>  R-squared:         </th> <td>   0.452</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.431</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   21.50</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 01 Oct 2017</td> <th>  Prob (F-statistic):</th> <td>9.68e-101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:28:23</td>     <th>  Log-Likelihood:    </th> <td> -781.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>   1636.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   963</td>      <th>  BIC:               </th> <td>   1818.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    37</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>  <td>    0.0095</td> <td>    0.017</td> <td>    0.552</td> <td> 0.581</td> <td>   -0.024</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>  <td>   -0.0333</td> <td>    0.017</td> <td>   -1.957</td> <td> 0.051</td> <td>   -0.067</td> <td> 9.59e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>  <td>   -0.0172</td> <td>    0.018</td> <td>   -0.979</td> <td> 0.328</td> <td>   -0.052</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>  <td>    0.0058</td> <td>    0.018</td> <td>    0.327</td> <td> 0.744</td> <td>   -0.029</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>  <td>    0.0146</td> <td>    0.004</td> <td>    3.729</td> <td> 0.000</td> <td>    0.007</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>  <td>    0.0023</td> <td>    0.018</td> <td>    0.131</td> <td> 0.896</td> <td>   -0.032</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>  <td>    0.0049</td> <td>    0.007</td> <td>    0.669</td> <td> 0.504</td> <td>   -0.009</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>  <td>    0.0091</td> <td>    0.007</td> <td>    1.291</td> <td> 0.197</td> <td>   -0.005</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>  <td>    0.0104</td> <td>    0.017</td> <td>    0.601</td> <td> 0.548</td> <td>   -0.024</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>  <td>    0.0152</td> <td>    0.017</td> <td>    0.885</td> <td> 0.377</td> <td>   -0.019</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th> <td>   -0.0049</td> <td>    0.017</td> <td>   -0.293</td> <td> 0.769</td> <td>   -0.038</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th> <td>   -0.0138</td> <td>    0.018</td> <td>   -0.783</td> <td> 0.434</td> <td>   -0.048</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12</th> <td>    0.0036</td> <td>    0.003</td> <td>    1.243</td> <td> 0.214</td> <td>   -0.002</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13</th> <td>    0.0192</td> <td>    0.017</td> <td>    1.111</td> <td> 0.267</td> <td>   -0.015</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14</th> <td>    0.0605</td> <td>    0.007</td> <td>    8.384</td> <td> 0.000</td> <td>    0.046</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th> <td>   -0.0273</td> <td>    0.017</td> <td>   -1.586</td> <td> 0.113</td> <td>   -0.061</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16</th> <td>    0.0045</td> <td>    0.018</td> <td>    0.252</td> <td> 0.801</td> <td>   -0.031</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17</th> <td>   -0.0163</td> <td>    0.017</td> <td>   -0.937</td> <td> 0.349</td> <td>   -0.050</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th> <td>    0.0262</td> <td>    0.008</td> <td>    3.118</td> <td> 0.002</td> <td>    0.010</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>19</th> <td>    0.0281</td> <td>    0.017</td> <td>    1.603</td> <td> 0.109</td> <td>   -0.006</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th> <td>    0.0028</td> <td>    0.018</td> <td>    0.157</td> <td> 0.875</td> <td>   -0.033</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21</th> <td>   -0.0075</td> <td>    0.018</td> <td>   -0.423</td> <td> 0.672</td> <td>   -0.042</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22</th> <td>    0.0276</td> <td>    0.007</td> <td>    4.164</td> <td> 0.000</td> <td>    0.015</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23</th> <td>    0.0017</td> <td>    0.004</td> <td>    0.444</td> <td> 0.657</td> <td>   -0.006</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24</th> <td>    0.0014</td> <td>    0.017</td> <td>    0.080</td> <td> 0.936</td> <td>   -0.032</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25</th> <td>    0.0011</td> <td>    0.017</td> <td>    0.064</td> <td> 0.949</td> <td>   -0.033</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>26</th> <td>    0.0115</td> <td>    0.017</td> <td>    0.663</td> <td> 0.508</td> <td>   -0.022</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>27</th> <td>   -0.0210</td> <td>    0.017</td> <td>   -1.249</td> <td> 0.212</td> <td>   -0.054</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28</th> <td>   -0.0093</td> <td>    0.008</td> <td>   -1.184</td> <td> 0.237</td> <td>   -0.025</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29</th> <td>   -0.0269</td> <td>    0.008</td> <td>   -3.353</td> <td> 0.001</td> <td>   -0.043</td> <td>   -0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>30</th> <td>    0.0118</td> <td>    0.017</td> <td>    0.690</td> <td> 0.490</td> <td>   -0.022</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>31</th> <td>    0.0101</td> <td>    0.017</td> <td>    0.583</td> <td> 0.560</td> <td>   -0.024</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>32</th> <td>   -0.0598</td> <td>    0.008</td> <td>   -7.951</td> <td> 0.000</td> <td>   -0.075</td> <td>   -0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>33</th> <td>   -0.0398</td> <td>    0.017</td> <td>   -2.351</td> <td> 0.019</td> <td>   -0.073</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th> <td>    0.0389</td> <td>    0.009</td> <td>    4.501</td> <td> 0.000</td> <td>    0.022</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>35</th> <td>    0.0047</td> <td>    0.017</td> <td>    0.272</td> <td> 0.786</td> <td>   -0.029</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>36</th> <td>   -0.0025</td> <td>    0.007</td> <td>   -0.348</td> <td> 0.728</td> <td>   -0.017</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>37</th> <td>    0.0094</td> <td>    0.017</td> <td>    0.540</td> <td> 0.589</td> <td>   -0.025</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>38</th> <td>   -0.0837</td> <td>    0.009</td> <td>   -9.466</td> <td> 0.000</td> <td>   -0.101</td> <td>   -0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>39</th> <td>    0.1366</td> <td>    0.008</td> <td>   16.369</td> <td> 0.000</td> <td>    0.120</td> <td>    0.153</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.074</td> <th>  Durbin-Watson:     </th> <td>   1.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.215</td> <th>  Jarque-Bera (JB):  </th> <td>   3.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.117</td> <th>  Prob(JB):          </th> <td>   0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.858</td> <th>  Cond. No.          </th> <td>1.01e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      0   R-squared:                       0.452\n",
       "Model:                            OLS   Adj. R-squared:                  0.431\n",
       "Method:                 Least Squares   F-statistic:                     21.50\n",
       "Date:                Sun, 01 Oct 2017   Prob (F-statistic):          9.68e-101\n",
       "Time:                        12:28:23   Log-Likelihood:                -781.19\n",
       "No. Observations:                1000   AIC:                             1636.\n",
       "Df Residuals:                     963   BIC:                             1818.\n",
       "Df Model:                          37                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "0              0.0095      0.017      0.552      0.581      -0.024       0.043\n",
       "1             -0.0333      0.017     -1.957      0.051      -0.067    9.59e-05\n",
       "2             -0.0172      0.018     -0.979      0.328      -0.052       0.017\n",
       "3              0.0058      0.018      0.327      0.744      -0.029       0.041\n",
       "4              0.0146      0.004      3.729      0.000       0.007       0.022\n",
       "5              0.0023      0.018      0.131      0.896      -0.032       0.037\n",
       "6              0.0049      0.007      0.669      0.504      -0.009       0.019\n",
       "7              0.0091      0.007      1.291      0.197      -0.005       0.023\n",
       "8              0.0104      0.017      0.601      0.548      -0.024       0.044\n",
       "9              0.0152      0.017      0.885      0.377      -0.019       0.049\n",
       "10            -0.0049      0.017     -0.293      0.769      -0.038       0.028\n",
       "11            -0.0138      0.018     -0.783      0.434      -0.048       0.021\n",
       "12             0.0036      0.003      1.243      0.214      -0.002       0.009\n",
       "13             0.0192      0.017      1.111      0.267      -0.015       0.053\n",
       "14             0.0605      0.007      8.384      0.000       0.046       0.075\n",
       "15            -0.0273      0.017     -1.586      0.113      -0.061       0.006\n",
       "16             0.0045      0.018      0.252      0.801      -0.031       0.040\n",
       "17            -0.0163      0.017     -0.937      0.349      -0.050       0.018\n",
       "18             0.0262      0.008      3.118      0.002       0.010       0.043\n",
       "19             0.0281      0.017      1.603      0.109      -0.006       0.062\n",
       "20             0.0028      0.018      0.157      0.875      -0.033       0.038\n",
       "21            -0.0075      0.018     -0.423      0.672      -0.042       0.027\n",
       "22             0.0276      0.007      4.164      0.000       0.015       0.041\n",
       "23             0.0017      0.004      0.444      0.657      -0.006       0.009\n",
       "24             0.0014      0.017      0.080      0.936      -0.032       0.035\n",
       "25             0.0011      0.017      0.064      0.949      -0.033       0.035\n",
       "26             0.0115      0.017      0.663      0.508      -0.022       0.045\n",
       "27            -0.0210      0.017     -1.249      0.212      -0.054       0.012\n",
       "28            -0.0093      0.008     -1.184      0.237      -0.025       0.006\n",
       "29            -0.0269      0.008     -3.353      0.001      -0.043      -0.011\n",
       "30             0.0118      0.017      0.690      0.490      -0.022       0.045\n",
       "31             0.0101      0.017      0.583      0.560      -0.024       0.044\n",
       "32            -0.0598      0.008     -7.951      0.000      -0.075      -0.045\n",
       "33            -0.0398      0.017     -2.351      0.019      -0.073      -0.007\n",
       "34             0.0389      0.009      4.501      0.000       0.022       0.056\n",
       "35             0.0047      0.017      0.272      0.786      -0.029       0.038\n",
       "36            -0.0025      0.007     -0.348      0.728      -0.017       0.012\n",
       "37             0.0094      0.017      0.540      0.589      -0.025       0.043\n",
       "38            -0.0837      0.009     -9.466      0.000      -0.101      -0.066\n",
       "39             0.1366      0.008     16.369      0.000       0.120       0.153\n",
       "==============================================================================\n",
       "Omnibus:                        3.074   Durbin-Watson:                   1.302\n",
       "Prob(Omnibus):                  0.215   Jarque-Bera (JB):                3.119\n",
       "Skew:                          -0.117   Prob(JB):                        0.210\n",
       "Kurtosis:                       2.858   Cond. No.                     1.01e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 3.96e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Note the difference in argument order\n",
    "ols = sm.OLS(Y1, X1)\n",
    "model = ols.fit()\n",
    "predictions = model.predict(X1) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=None, shuffle=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=2, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) # create an array\n",
    "y = np.array([1, 2, 3, 4]) # Create another array\n",
    "kf = KFold(n_splits=2) # Define the split - into 2 folds \n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "print(kf) \n",
    "KFold(n_splits=2, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [0 1] TEST: [2 3]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    " print 'TRAIN:', train_index, 'TEST:', test_index\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
